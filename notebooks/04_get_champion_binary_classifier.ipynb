{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b49b77da-3e4a-4b8e-acdb-76983fd48a70",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/competitions/amex-default-prediction/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf7f7d76-e043-46a2-9b57-3034303e2040",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import warnings\n",
    "import sys\n",
    "import json\n",
    "import gc\n",
    "import joblib  # Import joblib for model saving\n",
    "from io import StringIO\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ca664f4-8863-47a7-8100-67d374cf6ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.feature_selection import VarianceThreshold, RFE\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import xgboost as xgb\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.impute import SimpleImputer\n",
    "# from imblearn.under_sampling import RandomUnderSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a18d0e75-85bf-4ddb-80c6-43c8d71f9aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15a64be1-b2e6-4288-9482-ed5d41f5d2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_correlation_with_target(features_df, target_series, variance_threshold=0.0):\n",
    "    \"\"\"\n",
    "    Calculate the correlation of numeric columns in a features DataFrame with a target Series\n",
    "    and perform Variance Threshold feature selection.\n",
    "    \n",
    "    Parameters:\n",
    "    features_df (pd.DataFrame): The features DataFrame.\n",
    "    target_series (pd.Series): The target Series.\n",
    "    variance_threshold (float): Variance threshold for feature selection. Features with variance\n",
    "        below this threshold will be removed. Default is 0.0 (no threshold).\n",
    "\n",
    "    Returns:\n",
    "    pd.Series: A Series containing the correlation coefficients sorted by absolute values.\n",
    "    \"\"\"\n",
    "    # Select only numeric columns from the features DataFrame\n",
    "    numeric_features = features_df.select_dtypes(include=['number'])\n",
    "    \n",
    "    # Calculate the correlation and sort the result by absolute values in descending order\n",
    "    correlation_series = numeric_features.corrwith(target_series)\n",
    "    absolute_correlation_series = correlation_series.abs()\n",
    "    \n",
    "    # Apply Variance Threshold to filter features\n",
    "    if variance_threshold > 0.0:\n",
    "        selector = VarianceThreshold(threshold=variance_threshold)\n",
    "        numeric_features = selector.fit_transform(numeric_features)\n",
    "        # Update correlation series to match the selected features\n",
    "        correlation_series = pd.Series(selector.inverse_transform(correlation_series.values.reshape(1, -1))[0], index=numeric_features.columns)\n",
    "    \n",
    "    # Sort the DataFrame by absolute values\n",
    "    correlation_series = correlation_series.sort_values(ascending=False)\n",
    "    \n",
    "    return correlation_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a8e0118-5b17-43f7-9f55-31744869bc52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\KonuTech\\\\zoomcamp-capstone-01\\\\data'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the directory path\n",
    "data_dir = os.path.join('C:\\\\', 'Users', 'KonuTech', 'zoomcamp-capstone-01', 'data')\n",
    "data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82c245dd-e98d-4f19-8290-2cf73399cc3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_parquet_file = 'train_data_downsampled.parquet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f31126a4-3616-40df-b9d0-0f06c2f5734e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the training data\n",
    "train_data = pd.read_parquet(os.path.join(data_dir, train_data_parquet_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fea0b334-4971-4557-aa3b-b17ad5ff517a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 2755738 entries, 541332 to 2614482\n",
      "Data columns (total 191 columns):\n",
      " #    Column       Dtype  \n",
      "---   ------       -----  \n",
      " 0    customer_ID  object \n",
      " 1    S_2          object \n",
      " 2    P_2          float64\n",
      " 3    D_39         float64\n",
      " 4    B_1          float64\n",
      " 5    B_2          float64\n",
      " 6    R_1          float64\n",
      " 7    S_3          float64\n",
      " 8    D_41         float64\n",
      " 9    B_3          float64\n",
      " 10   D_42         float64\n",
      " 11   D_43         float64\n",
      " 12   D_44         float64\n",
      " 13   B_4          float64\n",
      " 14   D_45         float64\n",
      " 15   B_5          float64\n",
      " 16   R_2          float64\n",
      " 17   D_46         float64\n",
      " 18   D_47         float64\n",
      " 19   D_48         float64\n",
      " 20   D_49         float64\n",
      " 21   B_6          float64\n",
      " 22   B_7          float64\n",
      " 23   B_8          float64\n",
      " 24   D_50         float64\n",
      " 25   D_51         float64\n",
      " 26   B_9          float64\n",
      " 27   R_3          float64\n",
      " 28   D_52         float64\n",
      " 29   P_3          float64\n",
      " 30   B_10         float64\n",
      " 31   D_53         float64\n",
      " 32   S_5          float64\n",
      " 33   B_11         float64\n",
      " 34   S_6          float64\n",
      " 35   D_54         float64\n",
      " 36   R_4          float64\n",
      " 37   S_7          float64\n",
      " 38   B_12         float64\n",
      " 39   S_8          float64\n",
      " 40   D_55         float64\n",
      " 41   D_56         float64\n",
      " 42   B_13         float64\n",
      " 43   R_5          float64\n",
      " 44   D_58         float64\n",
      " 45   S_9          float64\n",
      " 46   B_14         float64\n",
      " 47   D_59         float64\n",
      " 48   D_60         float64\n",
      " 49   D_61         float64\n",
      " 50   B_15         float64\n",
      " 51   S_11         float64\n",
      " 52   D_62         float64\n",
      " 53   D_63         object \n",
      " 54   D_64         object \n",
      " 55   D_65         float64\n",
      " 56   B_16         float64\n",
      " 57   B_17         float64\n",
      " 58   B_18         float64\n",
      " 59   B_19         float64\n",
      " 60   D_66         float64\n",
      " 61   B_20         float64\n",
      " 62   D_68         float64\n",
      " 63   S_12         float64\n",
      " 64   R_6          float64\n",
      " 65   S_13         float64\n",
      " 66   B_21         float64\n",
      " 67   D_69         float64\n",
      " 68   B_22         float64\n",
      " 69   D_70         float64\n",
      " 70   D_71         float64\n",
      " 71   D_72         float64\n",
      " 72   S_15         float64\n",
      " 73   B_23         float64\n",
      " 74   D_73         float64\n",
      " 75   P_4          float64\n",
      " 76   D_74         float64\n",
      " 77   D_75         float64\n",
      " 78   D_76         float64\n",
      " 79   B_24         float64\n",
      " 80   R_7          float64\n",
      " 81   D_77         float64\n",
      " 82   B_25         float64\n",
      " 83   B_26         float64\n",
      " 84   D_78         float64\n",
      " 85   D_79         float64\n",
      " 86   R_8          float64\n",
      " 87   R_9          float64\n",
      " 88   S_16         float64\n",
      " 89   D_80         float64\n",
      " 90   R_10         float64\n",
      " 91   R_11         float64\n",
      " 92   B_27         float64\n",
      " 93   D_81         float64\n",
      " 94   D_82         float64\n",
      " 95   S_17         float64\n",
      " 96   R_12         float64\n",
      " 97   B_28         float64\n",
      " 98   R_13         float64\n",
      " 99   D_83         float64\n",
      " 100  R_14         float64\n",
      " 101  R_15         float64\n",
      " 102  D_84         float64\n",
      " 103  R_16         float64\n",
      " 104  B_29         float64\n",
      " 105  B_30         float64\n",
      " 106  S_18         float64\n",
      " 107  D_86         float64\n",
      " 108  D_87         float64\n",
      " 109  R_17         float64\n",
      " 110  R_18         float64\n",
      " 111  D_88         float64\n",
      " 112  B_31         int64  \n",
      " 113  S_19         float64\n",
      " 114  R_19         float64\n",
      " 115  B_32         float64\n",
      " 116  S_20         float64\n",
      " 117  R_20         float64\n",
      " 118  R_21         float64\n",
      " 119  B_33         float64\n",
      " 120  D_89         float64\n",
      " 121  R_22         float64\n",
      " 122  R_23         float64\n",
      " 123  D_91         float64\n",
      " 124  D_92         float64\n",
      " 125  D_93         float64\n",
      " 126  D_94         float64\n",
      " 127  R_24         float64\n",
      " 128  R_25         float64\n",
      " 129  D_96         float64\n",
      " 130  S_22         float64\n",
      " 131  S_23         float64\n",
      " 132  S_24         float64\n",
      " 133  S_25         float64\n",
      " 134  S_26         float64\n",
      " 135  D_102        float64\n",
      " 136  D_103        float64\n",
      " 137  D_104        float64\n",
      " 138  D_105        float64\n",
      " 139  D_106        float64\n",
      " 140  D_107        float64\n",
      " 141  B_36         float64\n",
      " 142  B_37         float64\n",
      " 143  R_26         float64\n",
      " 144  R_27         float64\n",
      " 145  B_38         float64\n",
      " 146  D_108        float64\n",
      " 147  D_109        float64\n",
      " 148  D_110        float64\n",
      " 149  D_111        float64\n",
      " 150  B_39         float64\n",
      " 151  D_112        float64\n",
      " 152  B_40         float64\n",
      " 153  S_27         float64\n",
      " 154  D_113        float64\n",
      " 155  D_114        float64\n",
      " 156  D_115        float64\n",
      " 157  D_116        float64\n",
      " 158  D_117        float64\n",
      " 159  D_118        float64\n",
      " 160  D_119        float64\n",
      " 161  D_120        float64\n",
      " 162  D_121        float64\n",
      " 163  D_122        float64\n",
      " 164  D_123        float64\n",
      " 165  D_124        float64\n",
      " 166  D_125        float64\n",
      " 167  D_126        float64\n",
      " 168  D_127        float64\n",
      " 169  D_128        float64\n",
      " 170  D_129        float64\n",
      " 171  B_41         float64\n",
      " 172  B_42         float64\n",
      " 173  D_130        float64\n",
      " 174  D_131        float64\n",
      " 175  D_132        float64\n",
      " 176  D_133        float64\n",
      " 177  R_28         float64\n",
      " 178  D_134        float64\n",
      " 179  D_135        float64\n",
      " 180  D_136        float64\n",
      " 181  D_137        float64\n",
      " 182  D_138        float64\n",
      " 183  D_139        float64\n",
      " 184  D_140        float64\n",
      " 185  D_141        float64\n",
      " 186  D_142        float64\n",
      " 187  D_143        float64\n",
      " 188  D_144        float64\n",
      " 189  D_145        float64\n",
      " 190  target       int64  \n",
      "dtypes: float64(185), int64(2), object(4)\n",
      "memory usage: 3.9+ GB\n"
     ]
    }
   ],
   "source": [
    "train_data.info(verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b85b3ce1-d5a5-4784-bbf5-ba696a7e3658",
   "metadata": {},
   "source": [
    "### splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e28e241e-4bd5-40b7-bb6d-f8c52fd6a120",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into train and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(train_data.iloc[:, :-1], train_data[\"target\"], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "320f27e8-91d3-4af9-9d1c-957142d024ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Explicitly call the garbage collector to free up memory\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "81fd1d6c-a85d-49da-9993-dba6bc037648",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2204590, 190)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "018722be-0299-4f96-96e4-046bbf95e829",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(551148, 190)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "684dea8e-d30f-47be-b950-16d6ce844361",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2204590,)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c3dce8a1-742e-4109-80c1-1396bb015e8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(551148,)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d8e2889c-d834-4eb8-8d98-0f4d6fe646ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\konutech\\zoomcamp-capstone-01\\venv\\lib\\site-packages\\numpy\\lib\\function_base.py:2897: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[:, None]\n",
      "c:\\users\\konutech\\zoomcamp-capstone-01\\venv\\lib\\site-packages\\numpy\\lib\\function_base.py:2898: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[None, :]\n"
     ]
    }
   ],
   "source": [
    "correlation_result = calculate_correlation_with_target(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "684b4378-2b6a-4ccc-8810-4d98a170d5c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['D_48',\n",
       " 'D_55',\n",
       " 'B_9',\n",
       " 'D_58',\n",
       " 'D_75',\n",
       " 'D_44',\n",
       " 'B_7',\n",
       " 'B_23',\n",
       " 'B_16',\n",
       " 'B_3',\n",
       " 'D_74',\n",
       " 'B_38',\n",
       " 'B_20',\n",
       " 'B_4',\n",
       " 'B_19']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_ten_correlations = list(correlation_result[:15].index)\n",
    "top_ten_correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "908f8be2-d1d7-4b24-b097-388c8940c2c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Excluding 'customer_ID', 'S_2' and creating a DictVectorizer\n"
     ]
    }
   ],
   "source": [
    "print(f\"Step 1: Excluding 'customer_ID', 'S_2' and creating a DictVectorizer\")\n",
    "\n",
    "# Exclude 'customer_ID' column and create a DictVectorizer\n",
    "dict_vectorizer = DictVectorizer(sparse=False)\n",
    "\n",
    "X_train_dict = X_train[top_ten_correlations].to_dict(orient='records')\n",
    "X_val_dict = X_val[top_ten_correlations].to_dict(orient='records')\n",
    "\n",
    "X_train_encoded = dict_vectorizer.fit_transform(X_train_dict)\n",
    "X_val_encoded = dict_vectorizer.transform(X_val_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "045d9c68-25d0-45cc-9c6f-8b3d11df3dba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dict_vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "19ab5bd7-fa0b-48eb-9e5b-c20e82e3b313",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['B_16', 'B_19', 'B_20', 'B_23', 'B_3', 'B_38', 'B_4', 'B_7', 'B_9',\n",
       "       'D_44', 'D_48', 'D_55', 'D_58', 'D_74', 'D_75'], dtype=object)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bfff2b92-1e59-4fc7-bda1-07ec8b421476",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([8.11689337e-03, 4.33485879e-03, 8.42621567e-04, 8.82114270e-03,\n",
       "       1.34561409e-02, 1.00000000e+00, 1.01981802e-03, 8.47173325e-03,\n",
       "       4.46140447e-03, 5.29108439e-03, 5.10770290e-02, 1.64400025e-01,\n",
       "       7.33613367e-04, 2.76507176e-04, 2.07962957e-03])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_encoded[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "78d7c29a-c66a-470a-b053-fec3606b1103",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dc2c0a5b-3354-422f-b440-c52fe00ba4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = [\n",
    "    ('LogisticRegression', LogisticRegression(n_jobs=-1), {\n",
    "        'classifier__C': [0.1, 1.0],\n",
    "        'classifier__penalty': ['l1', 'l2'],\n",
    "    }),\n",
    "    # ('RandomForest', RandomForestClassifier(n_jobs=-1), {\n",
    "    #     'classifier__n_estimators': [200],  # [100, 200],\n",
    "    #     'classifier__max_depth': [6],  # [None, 6],\n",
    "    #     'classifier__min_samples_split': [5],  # [2, 5], Optimized parameter\n",
    "    #     'classifier__min_samples_leaf': [2]  # [1, 2] Optimized parameter\n",
    "    # }),\n",
    "    ('XGBoost', xgb.XGBClassifier(n_jobs=-1), {\n",
    "        'classifier__n_estimators': [50, 100],\n",
    "        'classifier__max_depth': [3, 5],\n",
    "        'classifier__min_child_weight': [1, 2]  # Optimized parameter\n",
    "    })\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b31dcf31-68f2-40d6-b462-c0b1a6146541",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\konutech\\zoomcamp-capstone-01\\venv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:425: FitFailedWarning: \n",
      "6 fits failed out of a total of 12.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "6 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\users\\konutech\\zoomcamp-capstone-01\\venv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\users\\konutech\\zoomcamp-capstone-01\\venv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\users\\konutech\\zoomcamp-capstone-01\\venv\\lib\\site-packages\\sklearn\\pipeline.py\", line 427, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"c:\\users\\konutech\\zoomcamp-capstone-01\\venv\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\users\\konutech\\zoomcamp-capstone-01\\venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1169, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\users\\konutech\\zoomcamp-capstone-01\\venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 56, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\users\\konutech\\zoomcamp-capstone-01\\venv\\lib\\site-packages\\sklearn\\model_selection\\_search.py:979: UserWarning: One or more of the test scores are non-finite: [       nan 0.81305685        nan 0.81316889]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import json\n",
    "import gc\n",
    "import joblib\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from datetime import datetime\n",
    "\n",
    "# Set the log file name and log level\n",
    "log_file = 'training_log.log'\n",
    "log_level = logging.INFO  # You can change the log level as needed (e.g., DEBUG, INFO, WARNING, ERROR)\n",
    "\n",
    "# Generate a timestamp\n",
    "timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(filename=log_file, level=log_level, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "\n",
    "results = []\n",
    "\n",
    "for i, (name, classifier, params) in enumerate(classifiers, start=1):\n",
    "    logging.info(f\"Step {i}: Training {name} classifier\")\n",
    "\n",
    "    # Create an RFE model and a pipeline\n",
    "    rfe = RFE(estimator=classifier, n_features_to_select=10)  # select a number of features to be left\n",
    "\n",
    "    # Add a step to impute missing values with the median\n",
    "    imputer = SimpleImputer(strategy='median')  # You can choose a different strategy\n",
    "\n",
    "    pipeline = Pipeline([\n",
    "        ('imputer', imputer),  # Add the imputation step\n",
    "        ('feature_selection', rfe),\n",
    "        ('classifier', classifier)\n",
    "    ])\n",
    "\n",
    "    logging.info(f\"Step {i + 1}: Performing hyperparameter tuning\")\n",
    "\n",
    "    # Perform hyperparameter tuning\n",
    "    grid = GridSearchCV(pipeline, param_grid=params, cv=3, n_jobs=-1, verbose=3)\n",
    "    grid.fit(X_train_encoded, y_train)\n",
    "\n",
    "    # Access the feature names used in the best_estimator_\n",
    "    selected_feature_indices = grid.best_estimator_.named_steps['feature_selection'].get_support(indices=True)\n",
    "    logging.info(f\"Selected Indices of Features for {name}: {selected_feature_indices}\")\n",
    "\n",
    "    # Get selected features labels\n",
    "    # feature_names = X_train_encoded.columns\n",
    "    # Get the feature names from the DictVectorizer\n",
    "    feature_names = dict_vectorizer.get_feature_names_out()\n",
    "\n",
    "    # Get the selected feature names\n",
    "    selected_features = [feature_names[i] for i in selected_feature_indices]\n",
    "\n",
    "    logging.info(f\"Selected Features for {name}: {selected_features}\")\n",
    "\n",
    "    # Log the output of grid.best_estimator_\n",
    "    logging.info(f\"Best Estimator for {name}: {grid.best_estimator_}\")\n",
    "\n",
    "    # Save the best model as a .bin file with the timestamp\n",
    "    best_model = grid.best_estimator_\n",
    "    model_filename = f\"{name}_{timestamp}.bin\"  # Include the timestamp in the filename\n",
    "    joblib.dump(best_model, model_filename)\n",
    "\n",
    "    logging.info(f\"Step {i + 2}: Saving the best model for {name} as {model_filename}\")\n",
    "\n",
    "    # Explicit garbage collection\n",
    "    gc.collect()\n",
    "\n",
    "    logging.info(f\"Step {i + 3}: Evaluating the best model on the validation set using F1 score\")\n",
    "\n",
    "    # Evaluate the best model on the validation set using F1 score\n",
    "    y_pred = grid.predict(X_val_encoded)\n",
    "    f1 = f1_score(y_val, y_pred)\n",
    "\n",
    "    results.append({'name': name, 'f1': f1, 'best_params': grid.best_params_})\n",
    "\n",
    "# Create the JSON filename with the timestamp\n",
    "json_file_name = f'grid_search_results_{timestamp}.json'\n",
    "\n",
    "# Save the results to the JSON file\n",
    "with open(json_file_name, 'w') as file:\n",
    "    json.dump(results, file, indent=4)\n",
    "\n",
    "# Print the logs to the terminal\n",
    "log_formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "log_handler = logging.StreamHandler()\n",
    "log_handler.setFormatter(log_formatter)\n",
    "root_logger = logging.getLogger()\n",
    "root_logger.addHandler(log_handler)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
