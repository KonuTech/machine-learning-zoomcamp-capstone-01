2023-11-01 16:47:19,911 - root - INFO - Step 1: Training LogisticRegression classifier
2023-11-01 16:47:19,912 - root - INFO - Step 2: Performing hyperparameter tuning
2023-11-01 16:48:28,380 - root - INFO - Step 1: Training LogisticRegression classifier
2023-11-01 16:48:28,380 - root - INFO - Step 2: Performing hyperparameter tuning
2023-11-01 16:50:16,265 - root - INFO - Selected Indecies of Features for LogisticRegression: [1 2 3 4 5 6 9]
2023-11-01 16:50:16,266 - root - INFO - Selected Features for LogisticRegression: ['B_23', 'B_3', 'B_7', 'B_9', 'D_44', 'D_48', 'D_75']
2023-11-01 16:50:16,272 - root - INFO - Best Estimator for LogisticRegression: Pipeline(steps=[('imputer', SimpleImputer(strategy='median')),
                ('feature_selection',
                 RFE(estimator=LogisticRegression(n_jobs=-1),
                     n_features_to_select=7)),
                ('classifier', LogisticRegression(n_jobs=-1))])
2023-11-01 16:50:16,274 - root - INFO - Step 3: Saving the best model for LogisticRegression as LogisticRegression_best_model.bin
2023-11-01 16:50:16,364 - root - INFO - Step 4: Evaluating the best model on the validation set using F1 score
2023-11-01 16:50:16,693 - root - INFO - Step 2: Training XGBoost classifier
2023-11-01 16:50:16,693 - root - INFO - Step 3: Performing hyperparameter tuning
2023-11-01 16:57:59,153 - root - INFO - Selected Indecies of Features for XGBoost: [2 3 4 5 6 7 9]
2023-11-01 16:57:59,153 - root - INFO - Selected Features for XGBoost: ['B_3', 'B_7', 'B_9', 'D_44', 'D_48', 'D_55', 'D_75']
2023-11-01 16:57:59,166 - root - INFO - Best Estimator for XGBoost: Pipeline(steps=[('imputer', SimpleImputer(strategy='median')),
                ('feature_selection',
                 RFE(estimator=XGBClassifier(base_score=None, booster=None,
                                             callbacks=None,
                                             colsample_bylevel=None,
                                             colsample_bynode=None,
                                             colsample_bytree=None, device=None,
                                             early_stopping_rounds=None,
                                             enable_categorical=False,
                                             eval_metric=None,
                                             feature_types=None, gamma=None,
                                             grow_policy=None,
                                             i...
                               feature_types=None, gamma=None, grow_policy=None,
                               importance_type=None,
                               interaction_constraints=None, learning_rate=None,
                               max_bin=None, max_cat_threshold=None,
                               max_cat_to_onehot=None, max_delta_step=None,
                               max_depth=5, max_leaves=None, min_child_weight=2,
                               missing=nan, monotone_constraints=None,
                               multi_strategy=None, n_estimators=100, n_jobs=-1,
                               num_parallel_tree=None, random_state=None, ...))])
2023-11-01 16:57:59,176 - root - INFO - Step 4: Saving the best model for XGBoost as XGBoost_best_model.bin
2023-11-01 16:57:59,313 - root - INFO - Step 5: Evaluating the best model on the validation set using F1 score
2023-11-01 17:07:29,021 - root - INFO - Step 1: Training LogisticRegression classifier
2023-11-01 17:07:29,022 - root - INFO - Step 2: Performing hyperparameter tuning
2023-11-01 17:13:19,134 - root - INFO - Selected Indecies of Features for LogisticRegression: [ 2  3  4  6  7  8  9 10 13 14]
2023-11-01 17:13:19,136 - root - INFO - Selected Features for LogisticRegression: ['B_20', 'B_23', 'B_3', 'B_4', 'B_7', 'B_9', 'D_44', 'D_48', 'D_74', 'D_75']
2023-11-01 17:13:19,160 - root - INFO - Best Estimator for LogisticRegression: Pipeline(steps=[('imputer', SimpleImputer(strategy='median')),
                ('feature_selection',
                 RFE(estimator=LogisticRegression(n_jobs=-1),
                     n_features_to_select=10)),
                ('classifier', LogisticRegression(n_jobs=-1))])
2023-11-01 17:13:19,171 - root - INFO - Step 3: Saving the best model for LogisticRegression as LogisticRegression_best_model.bin
2023-11-01 17:13:25,587 - root - INFO - Step 4: Evaluating the best model on the validation set using F1 score
2023-11-01 17:13:25,969 - root - INFO - Step 2: Training XGBoost classifier
2023-11-01 17:13:25,969 - root - INFO - Step 3: Performing hyperparameter tuning
2023-11-01 17:25:38,739 - root - INFO - Selected Indecies of Features for XGBoost: [ 0  2  4  5  7  8  9 10 11 14]
2023-11-01 17:25:38,740 - root - INFO - Selected Features for XGBoost: ['B_16', 'B_20', 'B_3', 'B_38', 'B_7', 'B_9', 'D_44', 'D_48', 'D_55', 'D_75']
2023-11-01 17:25:38,754 - root - INFO - Best Estimator for XGBoost: Pipeline(steps=[('imputer', SimpleImputer(strategy='median')),
                ('feature_selection',
                 RFE(estimator=XGBClassifier(base_score=None, booster=None,
                                             callbacks=None,
                                             colsample_bylevel=None,
                                             colsample_bynode=None,
                                             colsample_bytree=None, device=None,
                                             early_stopping_rounds=None,
                                             enable_categorical=False,
                                             eval_metric=None,
                                             feature_types=None, gamma=None,
                                             grow_policy=None,
                                             i...
                               feature_types=None, gamma=None, grow_policy=None,
                               importance_type=None,
                               interaction_constraints=None, learning_rate=None,
                               max_bin=None, max_cat_threshold=None,
                               max_cat_to_onehot=None, max_delta_step=None,
                               max_depth=5, max_leaves=None, min_child_weight=2,
                               missing=nan, monotone_constraints=None,
                               multi_strategy=None, n_estimators=100, n_jobs=-1,
                               num_parallel_tree=None, random_state=None, ...))])
2023-11-01 17:25:38,763 - root - INFO - Step 4: Saving the best model for XGBoost as XGBoost_best_model.bin
2023-11-01 17:25:38,911 - root - INFO - Step 5: Evaluating the best model on the validation set using F1 score
2023-11-01 17:28:05,328 - root - INFO - Step 1: Training LogisticRegression classifier
2023-11-01 17:28:05,329 - root - INFO - Step 2: Performing hyperparameter tuning
2023-11-01 17:34:08,163 - root - INFO - Selected Indecies of Features for LogisticRegression: [ 2  3  4  6  7  8  9 10 13 14]
2023-11-01 17:34:08,165 - root - INFO - Selected Features for LogisticRegression: ['B_20', 'B_23', 'B_3', 'B_4', 'B_7', 'B_9', 'D_44', 'D_48', 'D_74', 'D_75']
2023-11-01 17:34:08,191 - root - INFO - Best Estimator for LogisticRegression: Pipeline(steps=[('imputer', SimpleImputer(strategy='median')),
                ('feature_selection',
                 RFE(estimator=LogisticRegression(n_jobs=-1),
                     n_features_to_select=10)),
                ('classifier', LogisticRegression(n_jobs=-1))])
2023-11-01 17:34:08,197 - root - INFO - Step 3: Saving the best model for LogisticRegression as LogisticRegression_best_model.bin
2023-11-01 17:34:14,648 - root - INFO - Step 4: Evaluating the best model on the validation set using F1 score
2023-11-01 17:34:15,055 - root - INFO - Step 2: Training XGBoost classifier
2023-11-01 17:34:15,055 - root - INFO - Step 3: Performing hyperparameter tuning
2023-11-01 17:46:45,732 - root - INFO - Selected Indecies of Features for XGBoost: [ 0  2  4  5  7  8  9 10 11 14]
2023-11-01 17:46:45,732 - root - INFO - Selected Features for XGBoost: ['B_16', 'B_20', 'B_3', 'B_38', 'B_7', 'B_9', 'D_44', 'D_48', 'D_55', 'D_75']
2023-11-01 17:46:45,748 - root - INFO - Best Estimator for XGBoost: Pipeline(steps=[('imputer', SimpleImputer(strategy='median')),
                ('feature_selection',
                 RFE(estimator=XGBClassifier(base_score=None, booster=None,
                                             callbacks=None,
                                             colsample_bylevel=None,
                                             colsample_bynode=None,
                                             colsample_bytree=None, device=None,
                                             early_stopping_rounds=None,
                                             enable_categorical=False,
                                             eval_metric=None,
                                             feature_types=None, gamma=None,
                                             grow_policy=None,
                                             i...
                               feature_types=None, gamma=None, grow_policy=None,
                               importance_type=None,
                               interaction_constraints=None, learning_rate=None,
                               max_bin=None, max_cat_threshold=None,
                               max_cat_to_onehot=None, max_delta_step=None,
                               max_depth=5, max_leaves=None, min_child_weight=2,
                               missing=nan, monotone_constraints=None,
                               multi_strategy=None, n_estimators=100, n_jobs=-1,
                               num_parallel_tree=None, random_state=None, ...))])
2023-11-01 17:46:45,759 - root - INFO - Step 4: Saving the best model for XGBoost as XGBoost_best_model.bin
2023-11-01 17:46:45,928 - root - INFO - Step 5: Evaluating the best model on the validation set using F1 score
2023-11-01 17:54:58,931 - root - INFO - Step 1: Training LogisticRegression classifier
2023-11-01 17:54:58,931 - root - INFO - Step 2: Performing hyperparameter tuning
2023-11-01 18:01:04,992 - root - INFO - Selected Indices of Features for LogisticRegression: [ 2  3  4  6  7  8  9 10 13 14]
2023-11-01 18:01:04,993 - root - INFO - Selected Features for LogisticRegression: ['B_20', 'B_23', 'B_3', 'B_4', 'B_7', 'B_9', 'D_44', 'D_48', 'D_74', 'D_75']
2023-11-01 18:01:05,016 - root - INFO - Best Estimator for LogisticRegression: Pipeline(steps=[('imputer', SimpleImputer(strategy='median')),
                ('feature_selection',
                 RFE(estimator=LogisticRegression(n_jobs=-1),
                     n_features_to_select=10)),
                ('classifier', LogisticRegression(n_jobs=-1))])
2023-11-01 18:01:05,023 - root - INFO - Step 3: Saving the best model for LogisticRegression as LogisticRegression_2023-11-01_17-54-58.bin
2023-11-01 18:01:11,715 - root - INFO - Step 4: Evaluating the best model on the validation set using F1 score
2023-11-01 18:01:12,027 - root - INFO - Step 2: Training XGBoost classifier
2023-11-01 18:01:12,028 - root - INFO - Step 3: Performing hyperparameter tuning
2023-11-01 18:14:03,486 - root - INFO - Selected Indices of Features for XGBoost: [ 0  2  4  5  7  8  9 10 11 14]
2023-11-01 18:14:03,486 - root - INFO - Selected Features for XGBoost: ['B_16', 'B_20', 'B_3', 'B_38', 'B_7', 'B_9', 'D_44', 'D_48', 'D_55', 'D_75']
2023-11-01 18:14:03,501 - root - INFO - Best Estimator for XGBoost: Pipeline(steps=[('imputer', SimpleImputer(strategy='median')),
                ('feature_selection',
                 RFE(estimator=XGBClassifier(base_score=None, booster=None,
                                             callbacks=None,
                                             colsample_bylevel=None,
                                             colsample_bynode=None,
                                             colsample_bytree=None, device=None,
                                             early_stopping_rounds=None,
                                             enable_categorical=False,
                                             eval_metric=None,
                                             feature_types=None, gamma=None,
                                             grow_policy=None,
                                             i...
                               feature_types=None, gamma=None, grow_policy=None,
                               importance_type=None,
                               interaction_constraints=None, learning_rate=None,
                               max_bin=None, max_cat_threshold=None,
                               max_cat_to_onehot=None, max_delta_step=None,
                               max_depth=5, max_leaves=None, min_child_weight=2,
                               missing=nan, monotone_constraints=None,
                               multi_strategy=None, n_estimators=100, n_jobs=-1,
                               num_parallel_tree=None, random_state=None, ...))])
2023-11-01 18:14:03,511 - root - INFO - Step 4: Saving the best model for XGBoost as XGBoost_2023-11-01_17-54-58.bin
2023-11-01 18:14:03,715 - root - INFO - Step 5: Evaluating the best model on the validation set using F1 score
2023-11-01 19:51:27,970 - root - INFO - Step 1: Training LogisticRegression classifier
2023-11-01 19:51:27,971 - root - INFO - Step 2: Performing hyperparameter tuning
2023-11-01 20:08:34,744 - root - INFO - Step 1: Training LogisticRegression classifier
2023-11-01 20:08:34,747 - root - INFO - Step 2: Performing hyperparameter tuning
2023-11-01 20:14:40,488 - root - INFO - Selected Indices of Features for LogisticRegression: [ 2  3  4  6  7  8  9 10 13 14]
2023-11-01 20:14:40,488 - root - INFO - Selected Features for LogisticRegression: ['B_20', 'B_23', 'B_3', 'B_4', 'B_7', 'B_9', 'D_44', 'D_48', 'D_74', 'D_75']
2023-11-01 20:14:40,496 - root - INFO - Best Estimator for LogisticRegression: Pipeline(steps=[('imputer', SimpleImputer(strategy='median')),
                ('feature_selection',
                 RFE(estimator=LogisticRegression(n_jobs=-1),
                     n_features_to_select=10)),
                ('classifier', LogisticRegression(n_jobs=-1))])
2023-11-01 20:14:40,499 - root - INFO - Step 3: Saving the best model for LogisticRegression as LogisticRegression_2023-11-01_20-08-34.bin
2023-11-01 20:14:40,649 - root - INFO - Step 4: Evaluating the best model on the validation set using Gini coefficient
2023-11-01 20:14:40,953 - root - INFO - Step 2: Training XGBoost classifier
2023-11-01 20:14:40,953 - root - INFO - Step 3: Performing hyperparameter tuning
2023-11-01 20:27:42,367 - root - INFO - Selected Indices of Features for XGBoost: [ 0  2  4  5  7  8  9 10 11 14]
2023-11-01 20:27:42,367 - root - INFO - Selected Features for XGBoost: ['B_16', 'B_20', 'B_3', 'B_38', 'B_7', 'B_9', 'D_44', 'D_48', 'D_55', 'D_75']
2023-11-01 20:27:42,382 - root - INFO - Best Estimator for XGBoost: Pipeline(steps=[('imputer', SimpleImputer(strategy='median')),
                ('feature_selection',
                 RFE(estimator=XGBClassifier(base_score=None, booster=None,
                                             callbacks=None,
                                             colsample_bylevel=None,
                                             colsample_bynode=None,
                                             colsample_bytree=None, device=None,
                                             early_stopping_rounds=None,
                                             enable_categorical=False,
                                             eval_metric=None,
                                             feature_types=None, gamma=None,
                                             grow_policy=None,
                                             i...
                               feature_types=None, gamma=None, grow_policy=None,
                               importance_type=None,
                               interaction_constraints=None, learning_rate=None,
                               max_bin=None, max_cat_threshold=None,
                               max_cat_to_onehot=None, max_delta_step=None,
                               max_depth=5, max_leaves=None, min_child_weight=2,
                               missing=nan, monotone_constraints=None,
                               multi_strategy=None, n_estimators=100, n_jobs=-1,
                               num_parallel_tree=None, random_state=None, ...))])
2023-11-01 20:27:42,392 - root - INFO - Step 4: Saving the best model for XGBoost as XGBoost_2023-11-01_20-08-34.bin
2023-11-01 20:27:42,587 - root - INFO - Step 5: Evaluating the best model on the validation set using Gini coefficient
