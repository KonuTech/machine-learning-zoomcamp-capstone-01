{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "082680a1-5cf5-49f1-b8d9-20033d1b9dc9",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/competitions/amex-default-prediction/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95c645e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "import psutil\n",
    "import pyarrow as pa\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "770cc3c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: C:\\Users\\KonuTech\\zoomcamp-capstone-01\\notebooks\n"
     ]
    }
   ],
   "source": [
    "current_directory = os.getcwd()\n",
    "print(\"Current working directory:\", current_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60159b94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 43G\n",
      "drwxr-xr-x 1 KonuTech 197121    0 Oct 30 01:25 .\n",
      "drwxr-xr-x 1 KonuTech 197121    0 Oct 29 19:44 ..\n",
      "drwxr-xr-x 1 KonuTech 197121    0 Oct 29 22:46 parquet_partitions\n",
      "-rw-r--r-- 1 KonuTech 197121  60M May 20  2022 sample_submission.csv\n",
      "-rw-r--r-- 1 KonuTech 197121  32G May 20  2022 test_data.csv\n",
      "-rw-r--r-- 1 KonuTech 197121  16G May 20  2022 train_data.csv\n",
      "-rw-r--r-- 1 KonuTech 197121 6.7G Oct 30 01:25 train_data.csv.zip\n",
      "-rw-r--r-- 1 KonuTech 197121 582M Oct 29 22:46 train_data.parquet\n",
      "-rw-r--r-- 1 KonuTech 197121  30M May 20  2022 train_labels.csv\n",
      "-rw-r--r-- 1 KonuTech 197121  27M Oct 29 22:42 train_labels.parquet\n"
     ]
    }
   ],
   "source": [
    "!ls -lah \"C:\\Users\\KonuTech\\zoomcamp-capstone-01\\data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f9a7d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get memory usage\n",
    "def get_memory_usage():\n",
    "    process = psutil.Process()\n",
    "    mem_info = process.memory_info()\n",
    "    return mem_info.rss / (1024 * 1024)  # Convert to megabytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94c7baeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\KonuTech\\\\zoomcamp-capstone-01\\\\data'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the directory path\n",
    "data_dir = os.path.join('C:\\\\', 'Users', 'KonuTech', 'zoomcamp-capstone-01', 'data')\n",
    "data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "758975b2-305c-4bd6-a217-d2f088a0013a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\KonuTech\\\\zoomcamp-capstone-01\\\\data\\\\parquet_partitions'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parquet_dir = os.path.join(data_dir, 'parquet_partitions')\n",
    "parquet_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "39abe865-0012-4dd2-ae6e-38df356fc349",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of file names to remove\n",
    "files_to_remove = ['train_data.parquet', 'train_labels.parquet']\n",
    "# Remove the files if they exist\n",
    "for file_name in files_to_remove:\n",
    "    file_path = os.path.join(data_dir, file_name)\n",
    "    if os.path.exists(file_path):\n",
    "        os.remove(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "efaebe6b-c1c2-402d-bfca-51add1523966",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all files in the directory\n",
    "file_list = os.listdir(parquet_dir)\n",
    "\n",
    "# Loop through the files and delete them\n",
    "for file_name in file_list:\n",
    "    file_path = os.path.join(parquet_dir, file_name)\n",
    "    if os.path.isfile(file_path):\n",
    "        os.remove(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc2269f2-3fe9-45a6-9ae0-d3128291c300",
   "metadata": {},
   "source": [
    "### train_labels.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2ff4be16-09b8-4b90-9888-ea88e660fccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV file (train_labels.csv)\n",
    "csv_file = 'train_labels.csv'\n",
    "train_labels_csv_file = os.path.join(data_dir, csv_file)\n",
    "train_labels = pd.read_csv(train_labels_csv_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "91bf5e40-8d9d-4687-a5fd-4e5139c27059",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the DataFrame to Parquet format\n",
    "parquet_file = 'train_labels.parquet'\n",
    "train_labels.to_parquet(f\"{data_dir}\\\\{parquet_file}\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "53c49296-d272-4733-bf2e-91127be92182",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels_parquet_file = os.path.join(data_dir, parquet_file)\n",
    "train_labels = pd.read_parquet(train_labels_parquet_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "67651c0a-f7a2-414d-bce2-9597378b5d11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 458913 entries, 0 to 458912\n",
      "Data columns (total 2 columns):\n",
      " #   Column       Non-Null Count   Dtype \n",
      "---  ------       --------------   ----- \n",
      " 0   customer_ID  458913 non-null  object\n",
      " 1   target       458913 non-null  int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 7.0+ MB\n"
     ]
    }
   ],
   "source": [
    "train_labels.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8a7a9cd6-9cb5-43ad-88ab-752b15eba1c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>458913.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.258934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.438050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              target\n",
       "count  458913.000000\n",
       "mean        0.258934\n",
       "std         0.438050\n",
       "min         0.000000\n",
       "25%         0.000000\n",
       "50%         0.000000\n",
       "75%         1.000000\n",
       "max         1.000000"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf5eb59-d29f-4bd9-b5e7-d7f1fde58c82",
   "metadata": {},
   "source": [
    "### train_data.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7b8ee9cf-0533-40fe-b041-9ecbd92d846b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Read the CSV file in chunks\n",
    "csv_file = 'train_data.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "57d69a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Initialize an empty list to store the Parquet partition file paths\n",
    "parquet_file_paths = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2d87a319-f63f-4e8e-9faa-38ac484fe5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = 100000  # Adjust the chunk size as needed\n",
    "i = 0  # Initialize the chunk number\n",
    "cumulative_rows = 0  # Initialize the cumulative row count\n",
    "\n",
    "if not os.path.exists(parquet_dir):\n",
    "    os.makedirs(parquet_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3f9e8a30-9423-4883-a1fa-e205c2e5fab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a TextFileReader, which is iterable with chunks of 10,000 rows.\n",
    "csv_iterator = pd.read_csv(os.path.join(data_dir, csv_file), iterator=True, chunksize=chunk_size)\n",
    "\n",
    "parquet_file_paths = []  # Initialize the list to store Parquet partition file paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "edc545b8-77b5-43a1-99a0-20d2d65a2bde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chunk 0, rows: 100000, cumulative rows: 100000\n",
      "Memory usage before chunk: 431.88 MB\n",
      "Memory usage after chunk: 454.71 MB\n",
      "Processing chunk 1, rows: 100000, cumulative rows: 200000\n",
      "Memory usage before chunk: 456.16 MB\n",
      "Memory usage after chunk: 472.36 MB\n",
      "Processing chunk 2, rows: 100000, cumulative rows: 300000\n",
      "Memory usage before chunk: 473.16 MB\n",
      "Memory usage after chunk: 490.67 MB\n",
      "Processing chunk 3, rows: 100000, cumulative rows: 400000\n",
      "Memory usage before chunk: 488.58 MB\n",
      "Memory usage after chunk: 507.42 MB\n",
      "Processing chunk 4, rows: 100000, cumulative rows: 500000\n",
      "Memory usage before chunk: 507.23 MB\n",
      "Memory usage after chunk: 493.61 MB\n",
      "Processing chunk 5, rows: 100000, cumulative rows: 600000\n",
      "Memory usage before chunk: 492.80 MB\n",
      "Memory usage after chunk: 509.95 MB\n",
      "Processing chunk 6, rows: 100000, cumulative rows: 700000\n",
      "Memory usage before chunk: 507.25 MB\n",
      "Memory usage after chunk: 525.20 MB\n",
      "Processing chunk 7, rows: 100000, cumulative rows: 800000\n",
      "Memory usage before chunk: 524.90 MB\n",
      "Memory usage after chunk: 527.30 MB\n",
      "Processing chunk 8, rows: 100000, cumulative rows: 900000\n",
      "Memory usage before chunk: 524.95 MB\n",
      "Memory usage after chunk: 526.29 MB\n",
      "Processing chunk 9, rows: 100000, cumulative rows: 1000000\n",
      "Memory usage before chunk: 525.27 MB\n",
      "Memory usage after chunk: 527.45 MB\n",
      "Processing chunk 10, rows: 100000, cumulative rows: 1100000\n",
      "Memory usage before chunk: 526.66 MB\n",
      "Memory usage after chunk: 543.38 MB\n",
      "Processing chunk 11, rows: 100000, cumulative rows: 1200000\n",
      "Memory usage before chunk: 542.43 MB\n",
      "Memory usage after chunk: 544.00 MB\n",
      "Processing chunk 12, rows: 100000, cumulative rows: 1300000\n",
      "Memory usage before chunk: 543.84 MB\n",
      "Memory usage after chunk: 561.39 MB\n",
      "Processing chunk 13, rows: 100000, cumulative rows: 1400000\n",
      "Memory usage before chunk: 559.66 MB\n",
      "Memory usage after chunk: 575.05 MB\n",
      "Processing chunk 14, rows: 100000, cumulative rows: 1500000\n",
      "Memory usage before chunk: 574.84 MB\n",
      "Memory usage after chunk: 591.58 MB\n",
      "Processing chunk 15, rows: 100000, cumulative rows: 1600000\n",
      "Memory usage before chunk: 589.94 MB\n",
      "Memory usage after chunk: 590.81 MB\n",
      "Processing chunk 16, rows: 100000, cumulative rows: 1700000\n",
      "Memory usage before chunk: 589.72 MB\n",
      "Memory usage after chunk: 606.55 MB\n",
      "Processing chunk 17, rows: 100000, cumulative rows: 1800000\n",
      "Memory usage before chunk: 605.27 MB\n",
      "Memory usage after chunk: 606.10 MB\n",
      "Processing chunk 18, rows: 100000, cumulative rows: 1900000\n",
      "Memory usage before chunk: 606.02 MB\n",
      "Memory usage after chunk: 591.02 MB\n",
      "Processing chunk 19, rows: 100000, cumulative rows: 2000000\n",
      "Memory usage before chunk: 590.06 MB\n",
      "Memory usage after chunk: 559.60 MB\n",
      "Processing chunk 20, rows: 100000, cumulative rows: 2100000\n",
      "Memory usage before chunk: 559.21 MB\n",
      "Memory usage after chunk: 574.98 MB\n",
      "Processing chunk 21, rows: 100000, cumulative rows: 2200000\n",
      "Memory usage before chunk: 574.98 MB\n",
      "Memory usage after chunk: 590.83 MB\n",
      "Processing chunk 22, rows: 100000, cumulative rows: 2300000\n",
      "Memory usage before chunk: 589.80 MB\n",
      "Memory usage after chunk: 590.89 MB\n",
      "Processing chunk 23, rows: 100000, cumulative rows: 2400000\n",
      "Memory usage before chunk: 590.17 MB\n",
      "Memory usage after chunk: 591.85 MB\n",
      "Processing chunk 24, rows: 100000, cumulative rows: 2500000\n",
      "Memory usage before chunk: 591.65 MB\n",
      "Memory usage after chunk: 576.02 MB\n",
      "Processing chunk 25, rows: 100000, cumulative rows: 2600000\n",
      "Memory usage before chunk: 575.03 MB\n",
      "Memory usage after chunk: 591.65 MB\n",
      "Processing chunk 26, rows: 100000, cumulative rows: 2700000\n",
      "Memory usage before chunk: 591.20 MB\n",
      "Memory usage after chunk: 591.62 MB\n",
      "Processing chunk 27, rows: 100000, cumulative rows: 2800000\n",
      "Memory usage before chunk: 592.31 MB\n",
      "Memory usage after chunk: 607.17 MB\n",
      "Processing chunk 28, rows: 100000, cumulative rows: 2900000\n",
      "Memory usage before chunk: 606.79 MB\n",
      "Memory usage after chunk: 607.38 MB\n",
      "Processing chunk 29, rows: 100000, cumulative rows: 3000000\n",
      "Memory usage before chunk: 606.39 MB\n",
      "Memory usage after chunk: 622.74 MB\n",
      "Processing chunk 30, rows: 100000, cumulative rows: 3100000\n",
      "Memory usage before chunk: 622.60 MB\n",
      "Memory usage after chunk: 638.95 MB\n",
      "Processing chunk 31, rows: 100000, cumulative rows: 3200000\n",
      "Memory usage before chunk: 638.22 MB\n",
      "Memory usage after chunk: 608.82 MB\n",
      "Processing chunk 32, rows: 100000, cumulative rows: 3300000\n",
      "Memory usage before chunk: 607.93 MB\n",
      "Memory usage after chunk: 624.51 MB\n",
      "Processing chunk 33, rows: 100000, cumulative rows: 3400000\n",
      "Memory usage before chunk: 624.58 MB\n",
      "Memory usage after chunk: 624.87 MB\n",
      "Processing chunk 34, rows: 100000, cumulative rows: 3500000\n",
      "Memory usage before chunk: 624.45 MB\n",
      "Memory usage after chunk: 625.29 MB\n",
      "Processing chunk 35, rows: 100000, cumulative rows: 3600000\n",
      "Memory usage before chunk: 623.73 MB\n",
      "Memory usage after chunk: 640.17 MB\n",
      "Processing chunk 36, rows: 100000, cumulative rows: 3700000\n",
      "Memory usage before chunk: 638.47 MB\n",
      "Memory usage after chunk: 639.84 MB\n",
      "Processing chunk 37, rows: 100000, cumulative rows: 3800000\n",
      "Memory usage before chunk: 639.07 MB\n",
      "Memory usage after chunk: 640.19 MB\n",
      "Processing chunk 38, rows: 100000, cumulative rows: 3900000\n",
      "Memory usage before chunk: 640.13 MB\n",
      "Memory usage after chunk: 639.23 MB\n",
      "Processing chunk 39, rows: 100000, cumulative rows: 4000000\n",
      "Memory usage before chunk: 639.37 MB\n",
      "Memory usage after chunk: 640.29 MB\n",
      "Processing chunk 40, rows: 100000, cumulative rows: 4100000\n",
      "Memory usage before chunk: 639.48 MB\n",
      "Memory usage after chunk: 640.69 MB\n",
      "Processing chunk 41, rows: 100000, cumulative rows: 4200000\n",
      "Memory usage before chunk: 639.36 MB\n",
      "Memory usage after chunk: 640.79 MB\n",
      "Processing chunk 42, rows: 100000, cumulative rows: 4300000\n",
      "Memory usage before chunk: 638.98 MB\n",
      "Memory usage after chunk: 608.78 MB\n",
      "Processing chunk 43, rows: 100000, cumulative rows: 4400000\n",
      "Memory usage before chunk: 607.96 MB\n",
      "Memory usage after chunk: 609.94 MB\n",
      "Processing chunk 44, rows: 100000, cumulative rows: 4500000\n",
      "Memory usage before chunk: 610.18 MB\n",
      "Memory usage after chunk: 594.10 MB\n",
      "Processing chunk 45, rows: 100000, cumulative rows: 4600000\n",
      "Memory usage before chunk: 593.32 MB\n",
      "Memory usage after chunk: 593.64 MB\n",
      "Processing chunk 46, rows: 100000, cumulative rows: 4700000\n",
      "Memory usage before chunk: 592.74 MB\n",
      "Memory usage after chunk: 593.73 MB\n",
      "Processing chunk 47, rows: 100000, cumulative rows: 4800000\n",
      "Memory usage before chunk: 593.20 MB\n",
      "Memory usage after chunk: 609.02 MB\n",
      "Processing chunk 48, rows: 100000, cumulative rows: 4900000\n",
      "Memory usage before chunk: 608.25 MB\n",
      "Memory usage after chunk: 592.57 MB\n",
      "Processing chunk 49, rows: 100000, cumulative rows: 5000000\n",
      "Memory usage before chunk: 591.72 MB\n",
      "Memory usage after chunk: 608.42 MB\n",
      "Processing chunk 50, rows: 100000, cumulative rows: 5100000\n",
      "Memory usage before chunk: 607.61 MB\n",
      "Memory usage after chunk: 608.66 MB\n",
      "Processing chunk 51, rows: 100000, cumulative rows: 5200000\n",
      "Memory usage before chunk: 607.66 MB\n",
      "Memory usage after chunk: 609.24 MB\n",
      "Processing chunk 52, rows: 100000, cumulative rows: 5300000\n",
      "Memory usage before chunk: 608.95 MB\n",
      "Memory usage after chunk: 610.68 MB\n",
      "Processing chunk 53, rows: 100000, cumulative rows: 5400000\n",
      "Memory usage before chunk: 609.16 MB\n",
      "Memory usage after chunk: 626.34 MB\n",
      "Processing chunk 54, rows: 100000, cumulative rows: 5500000\n",
      "Memory usage before chunk: 624.98 MB\n",
      "Memory usage after chunk: 626.18 MB\n",
      "Processing chunk 55, rows: 31451, cumulative rows: 5531451\n",
      "Memory usage before chunk: 525.41 MB\n",
      "Memory usage after chunk: 526.71 MB\n"
     ]
    }
   ],
   "source": [
    "# Iterate through the CSV file in chunks using pd.read_csv\n",
    "for chunk in csv_iterator:\n",
    "    # Display memory usage before reading the chunk\n",
    "    before_memory = get_memory_usage()\n",
    "\n",
    "    # Count and print the number of rows in the chunk\n",
    "    num_rows = len(chunk)\n",
    "    cumulative_rows += num_rows  # Accumulate the row count\n",
    "    print(f\"Processing chunk {i}, rows: {num_rows}, cumulative rows: {cumulative_rows}\")\n",
    "\n",
    "    # Save the chunk as a Parquet partition\n",
    "    parquet_partition_file = os.path.join(parquet_dir, f'chunk_{i}.parquet')\n",
    "    chunk.to_parquet(parquet_partition_file, index=False)\n",
    "\n",
    "    # Append the Parquet partition file path to the list\n",
    "    parquet_file_paths.append(parquet_partition_file)\n",
    "\n",
    "    # Display memory usage after reading and saving the chunk\n",
    "    after_memory = get_memory_usage()\n",
    "    print(f\"Memory usage before chunk: {before_memory:.2f} MB\")\n",
    "    print(f\"Memory usage after chunk: {after_memory:.2f} MB\")\n",
    "\n",
    "    i += 1  # Increment the chunk number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ddf913d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the Parquet partitions into a single DataFrame\n",
    "parquet_partitions = [pd.read_parquet(partition) for partition in parquet_file_paths]\n",
    "df = pd.concat(parquet_partitions, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d11008d-7a8a-412e-90b5-6dc78d0da349",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deduplicate the DataFrame by the \"customer_ID\" field\n",
    "# df = df.drop_duplicates([\"customer_ID\"])\n",
    "df = df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47d7bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Convert and save the combined DataFrame as a single Parquet file\n",
    "combined_parquet_file = 'train_data.parquet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30daecc6-f1a0-4120-8628-823d40010f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_parquet(os.path.join(data_dir, combined_parquet_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9602a827-b601-4de6-9efa-6488b20e1e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Remove individual Parquet partitions\n",
    "for partition_file in parquet_file_paths:\n",
    "    os.remove(partition_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f183f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: EDA on the combined Parquet file\n",
    "parquet_df = pd.read_parquet(os.path.join(data_dir, combined_parquet_file))  # Read the combined Parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4ad7cb-a240-4394-97c3-ee3cbcb88f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "parquet_df.info(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d3f5bf-04f0-499d-bea6-a16b81cc9dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "parquet_df[\"customer_ID\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a6d5975-0d33-439b-9d3c-be9e2ef3f8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "parquet_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e38706",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trigger garbage collection to clear unreferenced objects\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d094b2d6-152d-4ae6-9334-2f770f6b74b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
